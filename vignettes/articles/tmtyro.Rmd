---
title: "Introduction to tmtyro"
subtitle: "Simplified workflows for text-mining tyros"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to tmtyro}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(tmtyro)
```

Working with text as data is a multi-step process. After choosing and collecting documents, you'll need to load them in some structured way before anything else. Only then is it possible to "do" text analysis: tagging parts of speech, normalizing by lemma, comparing features, measuring sentiment, and so on. Even then, you'll need to communicate findings by preparing compelling explanations, tables, and visualizations of your results.

The tmtyro package aims to make these steps fast and easy.

* Purpose-built functions for collecting a corpus let you focus on *what* instead of *how*.
* Scalable functions for loading a corpus provide room for growth, from simple word count to grammar parsing and lemmatizing.
* Additional functions standardize approaches for measuring word use and vocabulary uniqueness, detecting sentiment, assessing term frequency--inverse document frequency, working with n-grams, and even building topic models.
* One simple function prepares publication-ready tables, automatically adjusting based on the kind of data used. Another simple function prepares compelling visualizations, returning clean, publication-ready figures.
* Every step is offered as a verb using complementary syntax. This keeps workflows easy to build, easy to understand, easy to explain, and easy to reproduce.

## Preparing texts 

tmtyro offers a few functions to gather and load texts for study:

* `get_gutenberg_corpus()` caches the HTML version of books by their Project Gutenberg ID, parses their text and headers, and presents them in a table.
* `get_micusp_corpus()` caches papers from the Michigan Corpus of Upper-level Student Papers, parses them for metadata and contents, and presents them in a table.
* `download_once()` caches an online file and passes the local path invisibly.
* `load_texts()` prepares a table in "tidytext" format with one word per row and columns for metadata. These texts can be loaded from a folder of files or passed from a table. Parameters allow for lemmatization, part-of-speech processing, and other options.

Other functions aid with preparing a corpus:

* `move_header_to_text()` corrects overzealous identification of HTML headers when parsing books from Project Gutenberg.
* `standardize_titles()` converts a vector or column into title case, converts underscores with spaces, and optionally removes initial articles.
* `identify_by()` sets a column of metadata to serve as document marker.

### Get a corpus

Collecting texts from Project Gutenberg will be a common first step for many. The function `get_gutenberg_corpus()` needs only the Gutenberg ID number, found in the book's URL. The resulting table draws metadata from the gutenbergr package, with columns for "gutenberg_id", "title", "author", headers such as those used for chapters, and "text."

```{r message=FALSE}
library(tmtyro)
joyce <- get_gutenberg_corpus(c(2814, 4217, 4300))

joyce
```

In some cases, headers may make better sense if read as part of the text, as in the "Aeolus" chapter of *Ulysses*, where frequent newspaper headlines pepper the page:

```{r}
ulysses <- get_gutenberg_corpus(4300)

# dplyr is used here to choose a smaller example for comparison
ulysses |> 
  dplyr::filter(section == "[ 7 ]")
```

These can be corrected with `move_header_to_text()`.

```{r}
ulysses <- get_gutenberg_corpus(4300) |> 
  move_header_to_text(subsection)

# dplyr is used here to choose a smaller example for comparison
ulysses |> 
  dplyr::filter(section == "[ 7 ]")
```

Headers can be moved for specific texts in a corpus by specifying a filter like `title == "Ulysses"`:

```{r}
joyce <- joyce |> 
  move_header_to_text(subsection, title == "Ulysses")

joyce |> 
  dplyr::filter(section == "[ 7 ]")
```

### Load texts

`load_texts()` prepares a set of documents for study, either from a table or from a folder of files.

#### From a table

A table like the one prepared by `get_gutenberg_corpus()` can be prepared in tidytext format with one word per row using `load_texts()`. 

```{r}
corpus_ulysses <- ulysses |> 
  load_texts()

corpus_ulysses
```

#### From files

If text files are already collected in a folder on disk, they can be prepared in a table by passing the path to the folder inside `load_texts()`. Used this way, `load_texts()` will load up every file using the "txt" file extension, populating the `doc_id` column with the first part of the file name.

```{r}
#| eval: false
corpus_austen <- load_texts("austen")
```

In this example, the "austen" folder is found within the current project. If it was instead found somewhere else on the computer, the complete path can be passed like this:<br> `load_texts("~/corpora/austen")`

### Choose a different `doc_id`

Documents loaded from `get_gutenberg_corpus()` use the `gutenberg_id` column as their document identifier. 

```{r}
corpus_dubliners <- get_gutenberg_corpus(2814) |> 
  load_texts(lemma = TRUE, pos = TRUE)

corpus_dubliners
```

If a different column is preferred, `identify_by()` makes the switch. In this example from *Dubliners*, for instance, each story's title is shown under "part". `identify_by()` makes it easy to identify documents by that column:

```{r}
corpus_dubliners <- corpus_dubliners |> 
  identify_by(part)

corpus_dubliners
```

### Standardize titles

`standardize_titles()` converts titles to something cleaner by adopting title case.

```{r}
before <- unique(corpus_dubliners$doc_id)

corpus_dubliners <- corpus_dubliners |> 
  standardize_titles()

after <- unique(corpus_dubliners$doc_id)

data.frame(before, after)
```

## Studying texts

Useful at many stages of work with a corpus, `contextualize()` shows the context of a search term, with an adjustable window on either side and options for searching with regular expressions. Most other functions for studying texts follow a predictable naming convention: 

* `add_frequency()` adds a column for word frequencies.
* `add_vocabulary()` adds columns measuring the lexical variety of texts.
* `add_sentiment()` adds a column of sentiment identifiers from a chosen lexicon.
* `add_ngrams()` adds columns of words for bigrams, trigrams, or more.

Not every method preserves the size or shape of data passed to it:

* `summarize_tf_idf()` returns a data frame for every token in each document in a corpus, with columns indicating weights for term frequency-inverse document frequency.

Along with these, other functions assist with the process:

* `drop_na()` drops rows with missing data in any column or in specified columns.
* `combine_ngrams()` combines multiple columns for n-grams into one.
* `separate_ngrams()` separatesa  single column of n-grams into one column per word.

But understanding context of key words with `show_context()` might be especially helpful.

### Showing context

`contextualize()` finds uses of a word within a corpus and returns a window of context around each use. 

```{r}
corpus_dubliners |> 
  contextualize("snow")
```

By default, `contextualize()` returns five results, showing a window of three words before and after an exact search term. Adjusting `limit` changes the number of results, with `limit = 0` returning a table. Other options include `window` to adjust the number of words shown and `regex` to accept partial matches.

```{r}
corpus_dubliners |> 
  contextualize(regex = "sno",
                window = 2,
                limit = 0)
```

When loading texts, `load_texts()` provides an option to keep original capitalization and punctuation. This option doesn't always work, and it seems incompatible with the current implementation of part-of-speech parsing, so it's not always appropriate. But using `contextualize()` on corpora loaded with `load_texts(keep_original = TRUE)` will show search terms much closer to their original context:

```{r}
corpus_joyce <- joyce |> 
  load_texts(keep_original = TRUE) |> 
  identify_by(title)

tundish <- 
  corpus_joyce |> 
  contextualize("tundish", limit = 1:7)
```

Even when `limit` is set to some value other than 0, the table of results is returned invisibly for later recall.

```{r}
tundish
```

### Word frequencies

`add_frequency()` adds word counts for each document in a new column, `n`.

```{r}
counts_dubliners <- 
  corpus_dubliners |> 
  add_frequency()

counts_dubliners
```

To count by some other column, indicate it in the parentheses:

```{r}
corpus_dubliners |> 
  add_frequency(lemma)
```

### Vocabulary richness

`add_vocabulary()` adds measurements of vocabulary richness, including cumulative vocabulary size, indicators of hapax legomena, and markers of progress.

```{r}
vocab_dubliners <- 
  corpus_dubliners |> 
  add_vocabulary()

vocab_dubliners
```

### Sentiment

`add_sentiment()` adds measurements of sentiment using the "Bing" lexicon by default.

```{r}
#| eval: false
sentiment_dubliners <- corpus_dubliners |> 
  add_sentiment()

sentiment_dubliners
```

```{r}
#| echo: false
if (interactive()) {
  sentiment_dubliners <- corpus_dubliners |> 
    add_sentiment()
  saveRDS(sentiment_dubliners, "sentiment_dubliners.Rds")
} else {
  sentiment_dubliners <- readRDS("sentiment_dubliners.Rds")
}

sentiment_dubliners
```

#### Dropping empty rows

Since many words may not be found in a given sentiment lexicon, `drop_na()` makes it easy to remove empty rows.

```{r}
sentiment_dubliners |> 
  drop_na(sentiment)
```

#### Choosing a sentiment lexicon

The lexicon can be chosen at measurement.

```{r}
#| eval: false
sentiment_ulysses <- ulysses |> 
  load_texts() |> 
  identify_by(section) |> 
  add_sentiment(lexicon = "nrc")

sentiment_ulysses |> 
  drop_na(sentiment)
```

```{r}
#| echo: false
if (interactive()) {
  sentiment_ulysses <- ulysses |> 
    load_texts() |> 
    identify_by(section) |> 
    add_sentiment(lexicon = "nrc")
  saveRDS(sentiment_ulysses, "sentiment_ulysses.Rds")
} else {
  sentiment_ulysses <- readRDS("sentiment_ulysses.Rds")
}

sentiment_ulysses |> 
  drop_na(sentiment)
```

### N-grams

Following the same pattern, `add_ngrams()` adds columns for n-length phrases of words. By default, it prepares bigrams (or 2-grams).

```{r}
bigrams_joyce <- corpus_joyce |> 
  add_ngrams()

bigrams_joyce
```

Other n-grams can be chosen by passing a vector of numbers.

```{r}
trigrams_joyce <- corpus_joyce |> 
  add_ngrams(1:3)

trigrams_joyce
```

### Tf-idf

`add_tf_idf()` adds measurements of term frequency (`tf`), inverse document frequency (`idf`), and combined term frequency--inverse document frequency (`tf_idf`).

```{r}
corpus_dubliners |> 
  add_tf_idf()
```

Term frequency--inverse document frequency is most commonly used as a way of summarizing language used, paying less attention word order and reducing documents to one instance of each token. To study texts this way, `summarize_tf_idf()` returns a table arranged in descending strength of tf-idf.

```{r}
tfidf_dubliners <- corpus_dubliners |> 
  summarize_tf_idf()

tfidf_dubliners
```

Tf-idf's method understandably emphasizes proper nouns that are unique to each document. The `remove_names` argument in `load_texts()` can help to filter out words that appear only in capitalized form. Removing names from *Dubliners* makes a noticeable difference in tf-idf results:

```{r}
tfidf_dubliners <- get_gutenberg_corpus(2814) |> 
  load_texts(remove_names = TRUE) |> 
  identify_by(part) |> 
  standardize_titles() |> 
  summarize_tf_idf()

tfidf_dubliners
```

If `load_texts()` is used with `pos = TRUE`, proper nouns can be filtered, but these tags are sometimes inaccurate.

## Preparing tables

`tabulize()` prepares tables for every kind of measurement. This repetition makes it easy to see and appreciate findings without struggling to recall a specialized function.

### Corpus details

By default, `tabulize()` prepares a table showing the lengths of each document.

```{r}
corpus_joyce |> 
  tabulize()
```


### Word frequencies

After `add_frequency()`, `tabulize()` will show the counts of the most-frequent words.

```{r}
corpus_joyce |> 
  add_frequency() |> 
  tabulize()
```

### Vocabulary richness

When used after `add_vocabulary()`, `tabulize()` prepares a clean summary table.

```{r}
corpus_joyce |> 
  add_vocabulary() |> 
  tabulize()
```

### Sentiment

For sentiment analysis, `tabulize()` returns a summary of figures for each document.

```{r}
# dplyr is used here to choose a smaller example for comparison
sentiment_dubliners_part <- sentiment_dubliners |> 
  dplyr::filter(doc_id %in% c("The Sisters", "An Encounter",  "Araby"))

sentiment_dubliners_part |> 
  tabulize()
```

Setting `drop_na = TRUE` removes rows without sentiment measure.

```{r}
sentiment_dubliners_part |> 
  tabulize(drop_na = TRUE)
```

The `ignore` parameter aids in selecting a subset of sentiments, converting the rest to `NA`.

```{r}
# dplyr is used here to choose a smaller example for comparison
sentiment_ulysses_part <- sentiment_ulysses |> 
  dplyr::filter(doc_id %in% c("[ 1 ]", "[ 2 ]", "[ 3 ]"))

sentiment_ulysses_part |> 
  tabulize(ignore = c("anger", "anticipation", "disgust", "fear", "trust", "positive", "negative"))
```

### N-grams

After `add_ngrams()`, `tabulize()` returns the top n-grams per document. By default, the first six are shown for each group, but rows can be chosen freely.

```{r}
bigrams_joyce |> 
  tabulize(rows = 1:2)
```

### Tf-idf

For data frames prepared with `add_tf_idf()` or `summarize_tf_idf()`, `tabulize()` returns six rows of the top-scoring words for each document. This amount can be adjusted with the `rows` argument.

```{r}
tfidf_dubliners |> 
  tabulize(rows = 1:3)
```


## Preparing figures

tmtyro provides many functions for preparing figures, but only one is typically needed:

* `visualize()` works intuitively with tmtyro objects, preparing figures suited to whatever work is being done. 

Customization is easy:

* `change_colors()` provides a single interface for modifying filled and colored layers.

### Corpus details

By default, `visualize()` prepares a figure showing the lengths of each document.

```{r}
corpus_joyce |> 
  visualize(inorder = FALSE)
```

### Word counts

Using `visualize()` after `add_frequency()` will chart the most frequent words of each document.

```{r}
corpus_joyce |> 
  add_frequency() |> 
  visualize()
```

`visualize()` takes additional arguments for customizing results.

```{r}
counts_dubliners |> 
  visualize(rows = 1:3, 
            color_y = TRUE, 
            reorder_y = TRUE)
```

### Vocabulary richness

When used after `add_vocabulary()`, `visualize()` charts each document by its length and the number of unique tokens. A figure like this is useful to compare documents by their rate of vocabulary growth.

```{r}
corpus_dubliners |> 
  add_vocabulary() |> 
  visualize()
```

Other features, such as type-token ratio ("ttr"), hapax introduction ratio ("hir"), or a sampling of hapax legomena ("hapax") can also be shown. 

```{r}
vocab_dubliners |> 
  visualize("ttr")
```

```{r}
corpus_joyce |> 
  add_vocabulary() |> 
  visualize("hapax")
```

### Sentiment

For sentiment analysis, `visualize()` allows for comparison among documents in a set.

```{r}
sentiment_dubliners |> 
  visualize()
```

The `ignore` parameter stipulates values to remove from the Y-axis to focus a figure.

```{r}
sentiment_ulysses |> 
  visualize(ignore = c("anger", "anticipation", "disgust", "fear", "trust", "positive", "negative"))
```

### N-grams

For n-grams, `visualize()` typically returns a network visualization inspired by the bigram network in *Text Mining with R*.

```{r}
#| message: false
bigrams_joyce |> 
  visualize()
```

### Combining n-grams

N-gram frequencies can be compared by combining them before visualization. Certain arguments allow for deviation from typical charts, including choosing the rows to chart and modifying colors to be set by values on the Y-axis.

```{r}
bigrams_joyce |> 
  dplyr::filter(word_1 == "he") |> 
  combine_ngrams() |> 
  visualize(rows = 1:5, color_y = TRUE)
```

### Tf-idf

`visualize()` returns bars showing the top words for each document. This can be a useful way to differentiate texts in a set from each other. Because `tfidf_dubliners` was prepared with `load_texts(remove_names = TRUE)`, the resulting chart shows clearer delineation of topics characteristic of the stories in Joyce's collection:

```{r}
tfidf_dubliners |> 
  visualize(rows = 1:4)
```

### Changing colors

`change_colors()` does what its name implies. By default, it adopts the "Dark2" palette from Brewer.

```{r}
sentiment_dubliners |> 
  visualize() |> 
  change_colors()
```

Colors can be chosen manually.

```{r}
#| message: false
library(ggraph)
bigrams_joyce |> 
  visualize(top_n = 60) |> 
  change_colors(c("#999999", 
                  "orange", 
                  "darkred"))
```

Optionally, use a named vector to set some colors by value instead of by order. By default unnamed colors are gray.

```{r}
bigrams_joyce |> 
  dplyr::filter(word_1 == "he") |> 
  combine_ngrams() |> 
  visualize(rows = 1:5, color_y = TRUE, reorder_y = TRUE) |> 
  change_colors(c(
    "he is" = "darkorange",
    "he has" = "orange"))
```

Unnamed colors fill in as needed.

```{r}
bigrams_joyce |> 
  dplyr::filter(word_1 == "he") |> 
  combine_ngrams() |> 
  visualize(rows = 1:5, color_y = TRUE, reorder_y = TRUE) |> 
  change_colors(c(
    "he is" = "darkorange",
    "he has" = "orange", 
    "navy", "skyblue"))
```

Or choose a predetermined color set and palette, as described in function documentation.

```{r}
tfidf_dubliners |> 
  visualize(rows = 1:4) |> 
  change_colors(colorset = "viridis", palette = "mako", direction = -1)
```
